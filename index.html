<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy">
    <meta name="keywords" content="Natural Language, Formal Method, Multi-Robots">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy</title>

    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>   -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-FV4ZJ9PVSV');
    </script>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-fullhd">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Scaling Up Natural Language Understanding for
                            Multi-Robots Through the Lens of Hierarchy</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="">Anonymous Authors</a>
                            </span>
                        </div>


                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a target="_blank" href="xxx.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper (Coming soon)</span>
                                    </a>
                                </span>

                                <!-- Arxiv Link. -->
                                <!-- <span class="link-block">
                                <a target="_blank" href="https://arxiv.org/abs/2307.05973333333333"
                                class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fas fa-file"></i>
                                </span>
                                <span>ArXiv</span>
                            </a>
                        </span> -->

                                <!-- Video Link. -->
                                <!-- <span class="link-block">
                            <a target="_blank" href="https://youtu.be/Yvn4eR05A3M"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                <i class="fab fa-youtube"></i>
                            </span>
                            <span>Video</span>
                        </a>
                    </span> -->

                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a target="_blank" href="https://github.com/"
                                        class="external-link button is-normal is-rounded is-dark"
                                        rel="noopener noreferrer">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (Coming soon)</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Long-horizon planning in robotics is often hindered by challenges such as uncertainty
                            accumulation, computational complexity, delayed rewards and incomplete information.
                            This work proposes an innovative approach to exploit the inherent task hierarchy from human
                            instructions to facilitate multi-robot planning.
                            Using Large Language Models (LLMs), we propose a two-step approach to translate
                            multi-sentence human instructions into a structured language,
                            Hierarchical Linear Temporal Logic (LTL), which serves as an intermediary formal
                            representation for planning.
                            Initially, LLMs transform the human instructions into a Hierarchical Task Network-like
                            representation, defined as Hierarchical Task Tree,
                            capturing the logical and temporal relations among tasks.
                            Following this, a domain-specific fine-tuning of LLM translates sub-tasks of each task into
                            flat LTL formulas,
                            aggregating them to form hierarchical LTL specifications.
                            These specifications are then leveraged for planning using off-the-shelf planners.
                            Our framework not only bridges the gap between human instructions and algorithmic planning
                            but also showcases the potential of LLMs in harnessing human-like hierarchical reasoning to
                            automate
                            complex task planning for multiple robots. Through evaluations in both simulation and
                            real-world
                            experiments involving human participants, we demonstrate that our method can handle more
                            complex human
                            instructions compared to existing methods. The results indicate that our approach achieves
                            higher success
                            rates and lower costs in multi-robot task allocation and plan generation.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->

        </div>

        <!-- Paper video. -->
        <br>
        <br>

        <!-- <div class="container is-max-widescreen">

            <div class="rows">
                <h2 class="title is-3">Video</h2>
                <div class="publication-video">
                    <iframe src="media/videos/supp_material.mp4" frameborder="0" allow="autoplay; encrypted-media"
                        allowfullscreen></iframe>
                </div>
            </div>
        </div> -->
        <div class="container is-max-widescreen">
            <div class="rows">
                <!-- <h2 class="title is-3">Video</h2> -->
                <div class="publication-video">
                    <iframe src="https://www.youtube.com/embed/ScuDko1a1DE" title="Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                </div>
            </div>
        </div>

    </section>

    <section class="section">
        <div class="container is-max-widescreen">

            <div class="rows">


                <!-- Animation. -->
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dperact">Overview</span> </h2>

                        <!-- Interpolating. -->
                        <div class="content has-text-justified">
                            <!-- <br> -->
                        </div>
                        <img src="media/figures/overview.png" class="interpolation-image" alt="overview image"/>
                        </br>
                        </br>
                        <p class="content has-text-justified">
                            <em>
                                Overview of the framework, using the dishwasher loading problem as a case study. Note that the non-leaf nodes in the Hierarchical Task Tree (HTT), the language descriptions of sibling tasks, and the flat specifications are color-coded to indicate one-to-one correspondence.
                            </em>
                            
                        </p>
                        <p class="content has-text-justified">                        
                            The HTT tree is structured such that it unfolds level by level, where each child task is a decomposition of its parent task. Notably, the tasks at the bottom level are not necessarily indecomposable. This flexibility allows for varying numbers of levels and tasks per level, accommodating differences in task understanding and the range of primitive actions available to robot agents.
                        </p>
                        <p class="content has-text-justified">    
                            Additionally, it's important to highlight that the relation $R$ specifically captures the temporal relationships between sibling tasks that share the same parent. The temporal relationship between any two tasks in the tree can be inferred by tracing their lineage back to their common ancestor, thereby simplifying the overall complexity of the structure. 
                        </p>
                        <p class="content has-text-justified">    
                            When a task instruction is received, we use LLMs to construct the HTT through a two-step process, as outlined in <b>step 1</b>.
                            <ol>
                                <li><strong>Logical search:</strong> For every non-leaf node $v$, we gather its child tasks $V'$ and the temporal relations among them, defined by $R' \subseteq V' \times V'$. We then use LLMs to rephrase these child tasks and their temporal relations into syntactically correct sentences aligned with the semantics of LTL specifications (as illustrated in <b>step 2.1</b>). These reformulated sentences are input into a fine-tuned LLM that produces a single LTL formula (as depicted in <b>step 2.2</b>).
                                </li>
                                <li><strong>Action Completion:</strong> Given an HTT, each leaf node should represent a simple task on certain object, such as <em>"task 1.1.1 place plates into the lower rack"</em> in example. By viewing such simple task as a sequence of action steps, we prompt the LLM to generate a sequence of pre-defined API calls to expand the simple task. For instance, the symbol $\pi_{\text{plates}}^l$ that represents task 1.1.1 can be replaced with LTL specification composed of sequential APIs (<b>step 2.2</b>): $$\pi_{\text{plates}}^l = \Diamond ( \texttt{Pickup(plate)} \wedge \Diamond\, \texttt{Move(plate, lower_rack)})$$
                                After this step, a complete hierarchical LTL specifications is generated (example in <b>step 2.1</b>).
                                </li>
                            </ol>

                        </p>

                        </br>
                        <!--/ Re-rendering. -->
                    </div>
                </div>
    </section>


    <section class="section">
        <div class="container is-max-widescreen">

            <div class="rows">
                <!-- <h2 class="title is-3"></h2> -->
                <h2 class="title is-3">AI2-THOR experiments</h2>
                <p class="content has-text-justified">
                    <span>
                        To evaluate our method on tasks with more complex temporal requirements, we combine several base tasks in the The ALFRED dataset to generate <em>derivative</em> tasks (each derivative task can be composed with up to 4 base tasks).

                    </span>
                    </br>
                    </br>
                    <span>
                        We compare our method with <a href="https://arxiv.org/abs/2309.10062">SMART-LLM</a>. SMART-LLM uses LLMs to generate Python scripts that invoke predefined APIs for the purposes of task decomposition and task allocation. 
                    </span>    
                    </br>
                    </br> 
                    <span>
                        The metrics are as follows: 
                        <ol>
                            <li>
                                <strong>Success rate</strong>, which measures whether the target goal states of objects are achieved and if the order in which these states occur satisfies the specified temporal requirements.
                            </li>
                            <li>
                                <strong>Travel cost</strong>, measured in meters, is defined as the total distance traveled by all robots, excluding any costs related to manipulation. 
                            </li>
                            <li>
                                <strong>Completion time</strong>, quantified as the number of discrete time steps required to complete the tasks.
                            </li>
    
                        </ol>
                    </br>
                    </br>
                    </span>

                
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-1task-1.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">1 base task with 4 robots</font>: Place a computer on the ottoman
                        </p>
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-1task-2.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">1 base task with 4 robots</font>: Pick up a green candle and place it on the countertop
                        </p>
                    </div>

                </div>
                <!-- <h2 class="title is-5">Meta-Control finds the most approriate representations and control strategies for heterogeneous robot skills</h2> -->

                <span>
                    For derivatives tasks with 1 base task, our methods can effectively choose the shortest path while avoiding the repeating actions when executing sub-tasks, compared with the SMART-LLM which primarily seeks to identify a feasible solution without focusing on optimizing costs.
                </span>
                </br>
                </br>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-1task-3.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">4 base tasks with 1 robot in dinning room</font>
                        </p>
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-4task-2.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">4 base tasks with 4 robots in dinning room</font>
                        </p>
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay width="99%">
                            <source src="media/videos/web-video-aithor-4task-3.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">4 base tasks with 4 robots in bedroom</font>
                        </p>
                    </div>
                </div>
                <span>
                    For derivative tasks with 4 base tasks, SMART-LLM failed to generate a feasible plan, as its prompt scripts always exceed the maximum content length the LLM accepts when the task content gradually becomes longer and more complex. However, as the HTT structure decomposes long tasks into tree-like dependencies, our method can effectively handle long tasks composed of multiple basic tasks, extract the temporal logic relationships, and provide planning. Here we demonstrated the planning of 4 robots under derivative tasks with 4 base tasks in two different environments.
                </span>
            </div>
        </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">

            <div class="rows">
                <!-- <h2 class="title is-3"></h2> -->
                <h2 class="title is-3">Real-world experiments</h2>
                <p class="content has-text-justified">
                    <span>
                        Our real-world experiments are conducted in a tabletop setting, where the task involves a robotic arm placing fruits and vegetables onto colored plates. 
                    </span>
                    </br>
                    </br>
                    <span>
                        Given the primarily 2D nature of the task, we convert the tabletop environment into a discrete grid world. The use of only one robotic arm simplifies the task compared to the multi-robot scenarios in the simulator, as it eliminates the need for task allocation. 

                    </span>
                    </br>
                    <span>
                        <ol>
                            Our evaluation focuses on two main aspects: 
                            <li>
                                the adaptability to different verbal tones and styles from various users;
                            </li>
                            <li>the comparative effectiveness of our plan solution against existing methods.

                            </li>
                        </ol>
                    </span>
                    
                    </br>
                    </br>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/5.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Wipe the whiteboard with a Cartesian trajectory planner.
                            Failed because force constraints can not be specified.
                        </p> -->
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/8.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Open the cabinet with a joint space planner. Failed
                            because planned swing path is not accurate.
                        </p> -->
                    </div>
                </div>
                <span>
                    same task with different object arrangements
                </span>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/6_1.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Wipe the whiteboard with a Cartesian trajectory planner.
                            Failed because force constraints can not be specified.
                        </p> -->
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/6_2.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Open the cabinet with a joint space planner. Failed
                            because planned swing path is not accurate.
                        </p> -->
                    </div>
                </div>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/7_1.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Wipe the whiteboard with a Cartesian trajectory planner.
                            Failed because force constraints can not be specified.
                        </p> -->
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/7_2.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Open the cabinet with a joint space planner. Failed
                            because planned swing path is not accurate.
                        </p> -->
                    </div>
                </div>
                <!-- <h2 class="title is-5">Meta-Control finds the most approriate representations and control strategies for heterogeneous robot skills</h2> -->

                <div class="columns">
                    <div class="column has-text-centered" >
                        <img src="media/figures/our_real_world.png" class="interpolation-image" style="width: 75%; height: auto;"/>
                    </div>
                </div>
                <span>
                    The figure above is a comparative overview of snapshots between our approach and LLMs for task <em>"Place the green apple in the pink plate, the orange in the yellow plate and the red apple in the blue plate. The order of placement is not specified and can be chosen freely"</em>. 
                </span>
                </p>
                </br>
                <span>
                    Our method gives an optimal plan based on the positions of the fruits, whereas LLMs basically follow the sequence in which the fruits are mentioned in the instructions.                
                </span>

                </br>

                </br>
                <div class="columns">
                    <div class="column has-text-centered" >
                        <img src="media/figures/table.jpg" class="interpolation-image" style="width: 75%; height: auto;"/>
                    </div>
                </div>                  
                <span>
                    As observed, both our approach and the LLM achieve a high success rate, which aligns with the expectations given the task complexities. This confirms that our method is capable of adapting to various user instructions. Regarding cost, for the first four tasks that require sequential actions, the costs are identical. For the latter four tasks, which allow for multiple feasible solutions, our method consistently produces lower-cost paths, with the exception of task 5. In this simple task, the LLM also manages to create an optimal plan based on the placement of fruits.

                </span>
            </div>
        </div>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                                href="https://voxposer.github.io/">VoxPoser</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>